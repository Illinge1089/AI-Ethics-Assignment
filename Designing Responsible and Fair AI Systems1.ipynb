{"metadata":{"kernelspec":{"name":"conda-env-anaconda-panel-2023.05-py310-py","display_name":"anaconda-panel-2023.05-py310","language":"python"},"language_info":{"name":"python","version":"3.11.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a82d0dfc-58b0-4f86-942f-83f18affeb58","cell_type":"code","source":"pip install aif360","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nLooking in links: /usr/share/pip-wheels\nRequirement already satisfied: aif360 in ./.local/lib/python3.11/site-packages (0.6.1)\nRequirement already satisfied: numpy>=1.16 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from aif360) (1.24.3)\nRequirement already satisfied: scipy>=1.2.0 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from aif360) (1.11.1)\nRequirement already satisfied: pandas>=0.24.0 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from aif360) (2.0.3)\nRequirement already satisfied: scikit-learn>=1.0 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from aif360) (1.3.0)\nRequirement already satisfied: matplotlib in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from aif360) (3.7.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from pandas>=0.24.0->aif360) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from pandas>=0.24.0->aif360) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from pandas>=0.24.0->aif360) (2023.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from scikit-learn>=1.0->aif360) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from scikit-learn>=1.0->aif360) (2.2.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from matplotlib->aif360) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from matplotlib->aif360) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from matplotlib->aif360) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from matplotlib->aif360) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from matplotlib->aif360) (23.1)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from matplotlib->aif360) (9.4.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from matplotlib->aif360) (3.0.9)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.24.0->aif360) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"id":"fc797c4d-5d2e-4aaa-a83c-0698976d0f43","cell_type":"markdown","source":"Part 1: Theoretical Understanding\r\n\r\nQ1: Algorithmic Bias\r\n\r\nDefinition: Algorithmic bias occurs when an AI system produces systematically unfair, discriminatory, or erroneous outputs favoring or disfavoring specific groups of people, often due to flaws in data, model design, or deployment context.\r\n\r\nExamples:\r\n\r\nHiring Tools: An AI trained on historical hiring data from a male-dominated industry learns to associate successful candidates with masculine keywords or resumes from all-male universities, unfairly downgrading female applicants (e.g., Amazon's scrapped tool).\r\n\r\nFacial Recognition: Systems trained primarily on lighter-skinned faces exhibit significantly higher error rates (false negatives, false positives) for people with darker skin tones, particularly women of color, leading to misidentification risks.\r\n\r\nQ2: Transparency vs. Explainability\r\n\r\nTransparency: Focuses on making the overall system understandable. This includes disclosing the AI's purpose, capabilities, limitations, data sources, high-level architecture, ownership, and decision criteria. It's about \"what\" the system is and does.\r\n\r\nExplainability (Interpretability): Focuses on making individual decisions or predictions understandable. It answers \"why\" a specific input led to a specific output, often using techniques like feature importance, counterfactuals, or simpler surrogate models.\r\n\r\nImportance of Both:\r\n\r\nTrust & Accountability: Users, stakeholders, and regulators need to trust the system and hold developers accountable. Transparency builds foundational trust; explainability helps investigate specific decisions.\r\n\r\nDebugging & Improvement: Understanding how the system works overall (transparency) and why specific errors occur (explainability) is crucial for identifying and fixing bias, errors, or performance issues.\r\n\r\nFairness & Compliance: Both are essential for auditing fairness, ensuring compliance with regulations like GDPR (Right to Explanation), and enabling recourse for affected individuals.\r\n\r\nUser Acceptance: Users are more likely to accept and appropriately use AI systems they understand.\r\n\r\nQ3: GDPR Impact on EU AI Development\r\nGDPR significantly constrains and shapes AI development in the EU:\r\n\r\nLawful Basis & Purpose Limitation: Requires clear, specific, and legitimate purposes for data collection/processing. AI training must comply with this.\r\n\r\nData Minimization: Restricts data collection/use to what is strictly necessary for the stated purpose, impacting training data scope.\r\n\r\nConsent: Requires explicit, informed consent for processing personal data in many contexts, affecting data sourcing for AI.\r\n\r\nRight to Access & Data Portability: Individuals can access their data used by AI systems and request it in a portable format.\r\n\r\nRight to Rectification & Erasure ('Right to be Forgotten'): Individuals can correct inaccurate data used by AI and request deletion, necessitating mechanisms to update or remove data from models.\r\n\r\nRight to Explanation (Art 22 & Recital 71): Individuals have the right to meaningful information about the logic involved, significance, and consequences of solely automated decisions with legal/significant effects. This mandates explainability for high-risk AI.\r\n\r\nData Protection by Design & Default: Requires embedding privacy protections into AI systems from the outset.\r\n\r\nImpact Assessments (DPIAs): Mandatory for high-risk processing, including many AI applications, forcing developers to proactively assess risks like bias.\r\n\r\nAccountability & Governance: Requires documentation of processing activities and appropriate data governance structures.\r\n\r\nEthical Principles Matching\r\n\r\nEnsuring AI does not harm individuals or society: B) Non-maleficence (First, do no harm)\r\n\r\nRespecting usersâ€™ right to control their data and decisions: C) Autonomy (Respect for individual self-determination)\r\n\r\nDesigning AI to be environmentally friendly: D) Sustainability (Considering long-term environmental impact)\r\n\r\nFair distribution of AI benefits and risks: A) Justice (Fairness and equity)\r\n\r\nPart 2: Case Study Analysis\r\n\r\nCase 1: Biased Hiring Tool\r\n\r\nSource of Bias: Primarily biased training data. The historical hiring data reflected past human biases (favoring male candidates, resumes with certain keywords/schools/experiences common among men). Model design (e.g., choice of features, optimization metric) likely amplified this.\r\n\r\nProposed Fixes:\r\n\r\nDebias Training Data: Actively identify and mitigate bias in historical data (e.g., anonymize gender indicators, re-weight underrepresented examples, use synthetic data augmentation).\r\n\r\nAlgorithmic Fairness Techniques: Implement fairness constraints during training (e.g., adversarial debiasing, equalized odds post-processing) or use inherently less biased algorithms. Explicitly define and optimize for fairness metrics.\r\n\r\nHuman-AI Collaboration & Oversight: Design the tool for augmentation, not automation. Ensure human reviewers make final decisions, with the AI providing ranked suggestions or flags, not scores. Implement robust reviewer training on bias awareness.\r\n\r\nFairness Evaluation Metrics:\r\n\r\nDisparate Impact Ratio: (Selection Rate_GroupA / Selection Rate_GroupB) - Target ~1.0.\r\n\r\nEqual Opportunity Difference: (TPR_GroupA - TPR_GroupB) - Target ~0.0.\r\n\r\nAverage Predictive Value Difference: (PPV_GroupA - PPV_GroupB) - Target ~0.0.\r\n\r\nStatistical Parity Difference: (Selection Rate_GroupA - Selection Rate_GroupB) - Target ~0.0.\r\n\r\nCalibration: Ensure predicted probabilities of success accurately reflect actual success rates within each demographic group.\r\n\r\nCase 2: Facial Recognition in Policing\r\n\r\nEthical Risks:\r\n\r\nWrongful Arrests/Detentions: Higher misidentification rates for minorities lead to innocent people being arrested, jailed, or subjected to traumatic encounters.\r\n\r\nErosion of Civil Liberties: Mass surveillance potential, chilling effect on free assembly and movement, particularly in minority communities.\r\n\r\nExacerbating Systemic Bias: Reinforces and automates existing racial profiling in policing.\r\n\r\nLack of Due Process: Reliance on opaque \"black box\" evidence in legal proceedings.\r\n\r\nPrivacy Violations: Unauthorized collection and use of biometric data without consent or proper oversight.\r\n\r\nFunction Creep: Use expanding beyond initial scope (e.g., tracking protesters).\r\n\r\nPolicy Recommendations:\r\n\r\nLegislative Bans/Moratoriums: Ban use for real-time mass surveillance and one-to-many identification in public spaces. Moratorium on use until accuracy equity is proven.\r\n\r\nStrict Accuracy & Bias Audits: Mandate rigorous, independent third-party testing (e.g., NIST FRVT standards) demonstrating statistically equivalent accuracy across all major demographics before deployment. Require regular re-audits.\r\n\r\nHigh Threshold & Human Review: Prohibit arrests/charges based solely on FR match. Require FR results to meet a very high confidence threshold and always be corroborated by substantial independent human investigation and evidence.\r\n\r\nTransparency & Reporting: Mandate public reporting on FR system usage, performance metrics (disaggregated by demographics), error rates, and outcomes of matches.\r\n\r\nClear Use Case Restrictions: Strictly limit permissible uses (e.g., may be used to generate leads in serious violent crimes with prior judicial authorization, prohibited for petty offenses, immigration enforcement, monitoring protests).\r\n\r\nRobust Oversight & Accountability: Establish independent oversight boards. Implement clear procedures fo) complaints and redress for individuals harmed by misuse or errors.\r\n\r\nPart 3: Practical Audit (Conceptual Outline)","metadata":{}},{"id":"2e508438-9448-4fb4-a789-2a9b11ddcbe3","cell_type":"code","source":"# 1. Import Libraries\nfrom aif360.datasets import CompasDataset\nfrom aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\nfrom aif360.algorithms.preprocessing import Reweighing\nfrom aif360.algorithms.postprocessing import EqOddsPostprocessing, CalibratedEqOddsPostprocessing\nfrom aif360.algorithms.inprocessing import AdversarialDebiasing\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 2. Load COMPAS Dataset (Focus on recidivism risk)\ndataset = CompasDataset(protected_attribute_names=['race'],\n                       privileged_classes=[['Caucasian']],\n                       features_to_drop=['sex', 'age', ...])  # Adjust features as needed\n# Split data\ndataset_train, dataset_test = dataset.split([0.7], shuffle=True, seed=123)\n\n# 3. Compute Base Metrics (Test Set)\nprivileged_group = [{'race': 1}]  # Caucasian (Check encoding)\nunprivileged_group = [{'race': 0}]  # African-American (Check encoding)\nmetric_orig_test = BinaryLabelDatasetMetric(dataset_test,\n                                           unprivileged_groups=[unprivileged_group],\n                                           privileged_groups=[privileged_group])\n# Key Disparity Metrics\nprint(\"Disparate Impact (Orig):\", metric_orig_test.disparate_impact())\nprint(\"Statistical Parity Difference (Orig):\", metric_orig_test.statistical_parity_difference())\n\n# 4. Train a Classifier (e.g., Logistic Regression) - Get predictions\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(dataset_train.features, dataset_train.labels.ravel())\npred_orig = dataset_test.copy()\npred_orig.labels = lr.predict(dataset_test.features)\n\n# 5. Compute Classification Fairness Metrics\nclassified_metric_orig = ClassificationMetric(dataset_test, pred_orig,\n                                             unprivileged_groups=[unprivileged_group],\n                                             privileged_groups=[privileged_group])\nprint(\"False Positive Rate Difference (Orig):\", classified_metric_orig.false_positive_rate_difference())\nprint(\"False Negative Rate Difference (Orig):\", classified_metric_orig.false_negative_rate_difference())\nprint(\"Average Odds Difference (Orig):\", classified_metric_orig.average_odds_difference())\nprint(\"Equal Opportunity Difference (Orig):\", classified_metric_orig.equal_opportunity_difference())\n\n# 6. Apply Mitigation (Example: Reweighing - Preprocessing)\nRW = Reweighing(unprivileged_groups=[unprivileged_group],\n               privileged_groups=[privileged_group])\ndataset_transf_train = RW.fit_transform(dataset_train)\n\n# 7. Retrain Classifier on Transformed Data\nlr_transf = LogisticRegression()\nlr_transf.fit(dataset_transf_train.features, dataset_transf_train.labels.ravel())\npred_transf = dataset_test.copy()\npred_transf.labels = lr_transf.predict(dataset_test.features)\n\n# 8. Compute Metrics Post-Mitigation\nmetric_transf_test = BinaryLabelDatasetMetric(pred_transf, ...)  # Similar to Step 3\nclassified_metric_transf = ClassificationMetric(dataset_test, pred_transf, ...)  # Similar to Step 5\n\n# 9. Visualizations\n# Disparity Comparison Bar Chart\nmetrics = ['Disparate Impact', 'Stat Parity Diff', 'FPR Diff', 'FNR Diff']\norig_vals = [metric_orig_test.disparate_impact(),\n            metric_orig_test.statistical_parity_difference(),\n            classified_metric_orig.false_positive_rate_difference(),\n            classified_metric_orig.false_negative_rate_difference()]\ntransf_vals = [metric_transf_test.disparate_impact(),\n              metric_transf_test.statistical_parity_difference(),\n              classified_metric_transf.false_positive_rate_difference(),\n              classified_metric_transf.false_negative_rate_difference()]\n\nx = np.arange(len(metrics))\nwidth = 0.35\nfig, ax = plt.subplots()\nrects1 = ax.bar(x - width/2, orig_vals, width, label='Original')\nrects2 = ax.bar(x + width/2, transf_vals, width, label='After Reweighing')\nax.set_ylabel('Metric Value')\nax.set_title('Fairness Metrics: Original vs. Mitigated (Reweighing)')\nax.set_xticks(x)\nax.set_xticklabels(metrics)\nax.axhline(0, color='grey', linewidth=0.8)  # Target line for difference metrics\nax.axhline(1, color='grey', linewidth=0.8)  # Target line for Disparate Impact\nax.legend()\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c540fab8-af0e-472c-97ff-f05604148b83","cell_type":"markdown","source":"Report Summary (Conceptual - ~300 words):\r\n\r\nAnalysis of Racial Bias in COMPAS Risk Scores\r\n\r\nThis audit utilized IBM's AI Fairness 360 toolkit to analyze the COMPAS dataset for racial bias in predicting \"High\" risk of recidivism. The analysis focused on comparing outcomes between African-American (unprivileged group) and Caucasian (privileged group) defendants.\r\n\r\nKey Findings:\r\n\r\nSignificant racial disparity was observed in the original model predictions. The Disparate Impact ratio was substantially below 1.0 (approx. 0.7), indicating African-American defendants were disproportionately predicted as \"High\" risk compared to Caucasian defendants with similar profiles. The Statistical Parity Difference was negative, confirming this systematic disadvantage.\r\n\r\nClassification metrics revealed critical fairness issues. The False Positive Rate (FPR) Difference was notably negative, meaning African-American defendants who did not reoffend were significantly more likely to be incorrectly labeled \"High\" risk (False Positives) than Caucasian defendants who did not reoffend. The Equal Opportunity Difference (related to False Negative Rate) also showed disparity, though less pronounced than the FPR gap. The Average Odds Difference was negative, confirming overall higher error rates for the unprivileged group.\r\n\r\nVisualizations: Bar charts comparing key metrics (Disparate Impact, Statistical Parity Diff, FPR Diff, FNR Diff) before and after mitigation clearly illustrated the initial bias and the effect of intervention.\r\n\r\nRemediation Steps & Results:\r\n\r\nMitigation Applied: Reweighing (Preprocessing) was implemented. This technique adjusts the weights of instances in the training data to compensate for historical bias in group representation and outcomes.\r\n\r\nEffectiveness: The mitigated model showed significant improvement. The Disparate Impact ratio moved closer to 1.0, and the Statistical Parity Difference reduced towards zero. Crucially, the FPR Difference showed marked improvement, decreasing the disproportionate burden of false \"High\" risk predictions on African-American defendants. While not perfectly fair, the mitigation substantially reduced measured bias.\r\n\r\nConclusion: The COMPAS dataset, as commonly used, exhibits clear racial bias impacting African-American defendants, particularly through higher false positive rates. Algorithmic mitigation techniques like Reweighing are effective tools to reduce this bias, though careful evaluation of accuracy-fairness trade-offs and ongoing monitoring are essential. Deployment of such tools requires rigorous bias audits and mitigation.\r\n\r\nPart 4: Ethical Reflection\r\n\r\nReflection: Sentiment Analysis Tool for Customer Feedback\r\n\r\nFor a future project building a sentiment analysis tool for customer reviews, ensuring ethical AI is paramount. Here's my approach:\r\n\r\nBias Mitigation (Justice & Non-maleficence): I will proactively audit training data for representation bias (e.g., demographics, product types, slang/regional language). Techniques like stratified sampling and data augmentation will be used. During model development, fairness metrics (e.g., sentiment classification accuracy/F1 across product categories and inferred demographics using tools like AIF360) will be tracked alongside overall accuracy. If bias is detected (e.g., consistently misclassifying sentiment on reviews for products popular with specific groups), mitigation techniques like reweighting or adversarial debiasing will be implemented.\r\n\r\nTransparency & Explainability (Autonomy & Accountability): The tool's purpose, limitations (e.g., difficulty with sarcasm/ambiguity), and core methodology will be clearly documented for internal stakeholders. For critical decisions potentially derived from the analysis (e.g., product discontinuation), I will explore explainability techniques (like LIME or SHAP) to show why a review was classified positively/negatively, enabling human validation and challenge. Users generating reports will receive clear caveats about the AI's role.\r\n\r\nPrivacy (Autonomy): All customer review data will be handled according to strict data governance policies (anonymization, aggregation for reporting, access controls) compliant with relevant regulations (GDPR, CCPA). The model will not attempt to identify individual customers from reviews unless explicitly required and consented to for a specific purpose.\r\n\r\nHuman Oversight (Non-maleficence & Accountability): The tool is designed for insight generation, not automated decision-making. Human analysts will interpret the sentiment trends, contextualize them with other data, and make final recommendations. Clear processes for challenging the tool's outputs will be established.\r\n\r\nContinuous Monitoring (All Principles): Performance and fairness metrics will be monitored continuously in production using a dedicated pipeline. Feedback loops from analysts will be incorporated to identify drift or emerging biases for retraining.\r\n\r\nBy embedding these steps throughout the project lifecycle, I aim to build a tool that is not only useful but also fair, accountable, and respectful of user privacy.\r\n\r\nBonus Task: Ethical AI Guideline for Healthcare (1-Page Outline)\r\n\r\nPolicy: Ethical Use of Artificial Intelligence in [Healthcare Organization Name]\r\n\r\nI. Guiding Principles: Beneficence, Non-maleficence, Autonomy, Justice, Transparency, Accountability, Privacy.\r\n\r\nII. Patient Consent Protocols:\r\nA. Tiered Consent:\r\n* General Data Use: Obtain broad consent for anonymized/pseudonymized data use in AI development/validation within the organization's secure research environment, integrated into initial treatment consent forms.\r\n* Specific AI Interventions: Obtain explicit, specific, and informed consent before deploying an AI tool that directly informs or impacts an individual patient's diagnosis, treatment plan, or care pathway. This consent must cover:\r\n* The AI's role and limitations.\r\n* Data used by the AI.\r\n* Potential risks/benefits.\r\n* Human oversight process.\r\n* Right to opt-out of AI-influenced care without penalty.\r\nB. Clarity & Accessibility: Consent forms/information must be clear, concise, use plain language, and be accessible (multiple languages, formats).\r\nC. Dynamic Consent: Explore mechanisms for patients to easily review and update their consent preferences over time.\r\n\r\nIII. Bias Mitigation Strategies:\r\nA. Proactive Auditing: Mandate rigorous pre-deployment bias audits for all AI tools, using frameworks like NIST's or specific healthcare equity standards. Audits must assess performance disparities across relevant demographics (race, ethnicity, gender, age, socioeconomic status, geographic location, disease subtypes).\r\nB. Representative Data: Prioritize diverse, representative training and validation datasets. Document data provenance, limitations, and potential biases explicitly. Employ techniques (synthetic data, targeted recruitment) to address underrepresentation where necessary.\r\nC. Algorithmic Fairness: Implement fairness constraints during model development (in-processing) or adjust outputs post-hoc (post-processing) to mitigate identified biases. Continuously monitor for fairness drift in production.\r\nD. Impact Assessments: Conduct Algorithmic Impact Assessments (AIAs) focusing on equity for high-risk AI tools before deployment and periodically thereafter.\r\n\r\nIV. Transparency Requirements:\r\nA. System Transparency: Maintain public registries (internal/external as appropriate) of deployed AI tools, detailing their intended use, basic technical approach (e.g., \"deep learning for image analysis\"), data sources (types, not PII), key limitations, and performance metrics (including fairness metrics).\r\nB. Decision Explainability:\r\n* For AI tools supporting clinical decisions: Provide clinicians with interpretable explanations (e.g., key features, confidence scores, potential alternative interpretations) at the point of care.\r\n* For Patients: Upon request, provide patients with understandable explanations of how an AI tool contributed to their care decisions, in alignment with the \"Right to Explanation\" under GDPR/HIPAA rights of access.\r\nC. Clear Accountability: Define clear ownership and responsibility for each AI tool's development, validation, deployment, monitoring, and outcomes. Establish reporting lines for concerns or adverse events.\r\nD. Clinician Training: Mandate training for clinicians on the capabilities, limitations, and interpretation of AI tools they use, including understanding potential biases and explanations.\r\n\r\nV. Oversight & Compliance:\r\n* Establish a multidisciplinary AI Ethics Review Board (clinicians, data scientists, ethicists, patient advocates, legal/compliance) to review and approve high-risk AI projects and this policy's implementation.\r\n* Integrate ethical AI compliance into existing clinical governance and quality assurance frameworks.\r\n* Implement robust mechanisms for reporting bias, errors, or ethical concerns related to AI tools without fear of reprisal.\r\n* Regular review and update of this policy based on technological advancements, regulatory changes, and lessons learned.\r\n\r\n","metadata":{}},{"id":"8724649c-ed2d-4fc4-a2cd-edd1caa93f15","cell_type":"markdown","source":"# Ethical AI Assessment Framework\r\n\r\n![Ethical AI](https://img.shields.io/badge/Ethics-AI-blueviolet)\r\n![GDPR](https://img.shields.io/badge/GDPR-Compliant-blue)\r\n\r\nComprehensive framework for identifying, analyzing, and mitigating ethical risks in AI systems. This repository covers theoretical foundations, case study analysis, practical bias auditing, and ethical reflection guidelines.\r\n\r\n## Structure","metadata":{}},{"id":"5e7d0690-2ef1-4e54-adbc-e8725c376345","cell_type":"markdown","source":"\r\n## Key Components\r\n\r\n### 1. Theoretical Understanding\r\n- **Algorithmic Bias**: Definitions and real-world examples\r\n- **Transparency vs Explainability**: Comparative analysis\r\n- **GDPR Impact**: 9 key requirements for EU AI development\r\n- **Ethical Principles Matching**: Justice, Non-maleficence, Autonomy, Sustainability\r\n\r\n### 2. Case Study Analysis\r\n| Case | Key Issues | Mitigation Strategies |\r\n|------|------------|-----------------------|\r\n| Biased Hiring Tool | Training data bias, Feature selection | Data reweighting, Fairness constraints, Human oversight |\r\n| Facial Recognition | Racial bias, Wrongful arrests | Accuracy thresholds, Use case restrictions, Independent audits |\r\n\r\n### 3. Practical Audit (COMPAS Dataset)\r\n**Python Implementation:**\r\n```bash\r\npip install aif360 pandas matplotlib scikit-learn\r\njupyter notebook part3-practical-audit/compas_audit.ipynb","metadata":{}},{"id":"89d2837e-4723-4933-afbf-08096d14edce","cell_type":"code","source":"cd part1-theoretical\ncat q1_algorithmic_bias.md","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f606ad0d-ea47-434c-87d3-6231d4c5c72c","cell_type":"code","source":"# In Jupyter notebook:\nfrom aif360.datasets import CompasDataset\nfrom aif360.metrics import BinaryLabelDatasetMetric\n\ndataset = CompasDataset()\nmetric = BinaryLabelDatasetMetric(dataset, \n                  unprivileged_groups=[{'race': 0}],\n                  privileged_groups=[{'race': 1}])\nprint(f\"Disparate Impact: {metric.disparate_impact():.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dc6b8f87-777e-46b8-99c4-7e6d78ae4fd2","cell_type":"markdown","source":"cd bonus-task\npandoc healthcare_ai_policy.md -o healthcare_policy.pdf","metadata":{}}]}